import os
import operator
from dotenv import load_dotenv
from typing import TypedDict, Annotated, List

# LangChain e LangGraph
from langchain_core.messages import BaseMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain.memory import ChatMessageHistory
from langgraph.graph import END, StateGraph


from AgenteLang.categorias_intencao import categorias_intencao

load_dotenv()

# Modelo de linguagem
llm = ChatOpenAI(model='gpt-4o', temperature=0)

# --- ESTADO CORRIGIDO ---
class AgentState(TypedDict):
    """
    Define os estados do agente.
    'history' agora usa a sintaxe correta para acumular mensagens.
    """
    input: str
    intencao: str
    resposta_final: str
    # A forma correta de definir um hist√≥rico que se acumula no LangGraph
    history: Annotated[List[BaseMessage], operator.add]

# --- N√ìS CORRIGIDOS PARA USAR A MEM√ìRIA ---

def classificar_intencao(state: AgentState) -> dict:
    """
    Classifica a inten√ß√£o do usu√°rio usando o hist√≥rico para contexto.
    """
    print("--- üß† Classificando inten√ß√£o ---")
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© um classificador de inten√ß√£o. Sua tarefa √© analisar a √∫ltima mensagem do usu√°rio, usando o hist√≥rico da conversa para contexto, e classific√°-la em uma das seguintes categorias:
        {categorias}
        Responda APENAS com o nome da categoria."""),
        MessagesPlaceholder(variable_name="history"), # Onde o hist√≥rico √© inserido
    ])
    chain = prompt | llm | StrOutputParser()
    intencao_classificada = chain.invoke({
        "history": state['history'],
        "categorias": ", ".join(categorias_intencao)
    })
    return {"intencao": intencao_classificada}

def responder_busca_info(state: AgentState) -> dict:
    """
    N√≥ especialista que usa o hist√≥rico para responder perguntas.
    """
    print("--- üîç Respondendo busca de informa√ß√£o (com mem√≥ria) ---")
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um assistente de pesquisa prestativo. Use o hist√≥rico da conversa para responder a pergunta do usu√°rio de forma completa e contextual."),
        MessagesPlaceholder(variable_name="history"),
    ])
    chain = prompt | llm | StrOutputParser()
    resposta = chain.invoke({"history": state['history']})
    return {"resposta_final": resposta}

def responder_saudacao(state: AgentState) -> dict:
    """
    N√≥ especialista que usa o hist√≥rico para responder cumprimentos.
    """
    print("--- üëã Respondendo sauda√ß√£o (com mem√≥ria) ---")
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um assistente amig√°vel e educado. Use o hist√≥rico para responder ao cumprimento do usu√°rio de forma simp√°tica e contextual."),
        MessagesPlaceholder(variable_name="history"),
    ])
    chain = prompt | llm | StrOutputParser()
    resposta = chain.invoke({"history": state['history']})
    return {"resposta_final": resposta}

# --- L√ìGICA DE ROTEAMENTO (sem altera√ß√µes) ---
def decidir_proximo_passo(state: AgentState) -> str:
    print(f"--- üõ§Ô∏è Decidindo rota com base na inten√ß√£o: {state['intencao']} ---")
    if "BUSCA_INFO" in state['intencao']:
        return "node_busca_info"
    else:
        return "node_saudacao"

# --- MONTAGEM DO GRAFO (sem altera√ß√µes) ---
graph = StateGraph(AgentState)
graph.add_node('classificador', classificar_intencao)
graph.add_node('node_busca_info', responder_busca_info)
graph.add_node('node_saudacao', responder_saudacao)
graph.set_entry_point('classificador')
graph.add_conditional_edges(
    'classificador',
    decidir_proximo_passo,
    {"node_busca_info": "node_busca_info", "node_saudacao": "node_saudacao"}
)
graph.add_edge('node_busca_info', END)
graph.add_edge('node_saudacao', END)
app = graph.compile()

# --- LOOP DE CONVERSA CORRIGIDO ---

# 1. A mem√≥ria √© inicializada FORA do loop
history = ChatMessageHistory()

while True:
    input_usuario = input("Voc√™: ")
    if input_usuario.lower() in ['sair', 'exit']:
        break

    # 2. Adiciona a nova mensagem do usu√°rio ao hist√≥rico
    history.add_user_message(input_usuario)

    # 3. Invoca o grafo com o hist√≥rico completo
    # O grafo internamente adicionar√° a resposta da IA ao estado
    resultado = app.invoke({"history": history.messages, "input": input_usuario})
    
    resposta_agente = resultado['resposta_final']

    # 4. Adiciona a resposta do agente ao hist√≥rico para a pr√≥xima rodada
    history.add_ai_message(resposta_agente)
    
    print(f"Agente: {resposta_agente}")